{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Programming\n",
    "\n",
    "An optimization problem with a quadratic objective function and linear\n",
    "constraints is called a _quadratic program_. Problems of this\n",
    "type are important in their own right, and they also arise a subproblems\n",
    "in methods for general constrained optimization such as sequential\n",
    "quadratic programming (this chapter) and interior-point methods (lecture 10).\n",
    "\n",
    "The general quadratic program can be stated as\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\vec{x}}\\, & f\\left(\\vec{x}\\right)\\overset{\\vartriangle}{=}\\frac{1}{2}\\vec{x}^\\mathsf{T}Q\\vec{x}-\\vec{c}^\\mathsf{T}\\vec{x}\\\\\n",
    "\\textrm{subject to}\\, & \\begin{cases}\n",
    "A_{\\textrm{eq}}\\vec{x}=\\vec{b}_{\\textrm{eq}}\\,,\\\\\n",
    "A_{\\textrm{in}}\\vec{x}\\leq\\vec{b}_{\\textrm{in}}\\,,\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where $Q$ is a symmetric $n\\times n$ matrix, $\\vec{c}\\in\\mathbb R^{n}$,\n",
    "$A_{\\textrm{eq}}$ is a $m\\times n$ matrix, $\\vec{b}_{\\textrm{eq}}\\in\\mathbb R^{m}$,\n",
    "$A_{\\textrm{in}}$ is a $p\\times n$ matrix and $\\vec{b}_{\\textrm{in}}\\in\\mathbb R^{p}$.\n",
    "If the Hessian matrix $Q$ is positive semidefinite, we have a convex\n",
    "quadratic program. Non-convex quadratic programs, in which $Q$ is\n",
    "an indefinite matrix, can be more challenging because they have several\n",
    "stationary points and local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equality Constraints\n",
    "\n",
    "We begin our discussion of algorithms for quadratic programming by considering\n",
    "the case in which only equality constraints are present. We consider\n",
    "to following equality-constrained quadratic problem\n",
    "\\begin{align}\n",
    "\\min_{\\vec{x}}\\, & f\\left(\\vec{x}\\right)\\overset{\\vartriangle}{=}\\frac{1}{2} \\vec{x}^\\mathsf{T}Q\\vec{x}- \\vec{c}^\\mathsf{T}\\vec{x}\\\\\n",
    "\\textrm{subject to}\\, & A\\vec{x}=\\vec{b}\\,,\\nonumber \n",
    "\\end{align}\n",
    "where $A$ is the $m\\times n$ Jacobian of constraints and $\\vec{b}\\in\\mathbb R^{m}$.\n",
    "We assume that $A$ has rank $m$ so that the constraints are consistent.\n",
    "\n",
    "The First Order Necessary Condition (FONC) uses the Langrangian function\n",
    "\\begin{equation}\n",
    "\\mathcal L\\left(\\vec x, \\vec \\lambda\\right)=\\vec{x}^\\mathsf{T}Q\\vec{x}- \\vec{c}^\\mathsf{T}\\vec{x} + \\vec\\lambda^\\mathsf{T} \\left(A\\vec{x}-\\vec{b}\\right)\n",
    "\\end{equation}\n",
    "and gives the following condition\n",
    "\\begin{align}\n",
    "\\nabla_{\\vec x}\\mathcal L\\left(\\vec x ^\\star, \\vec \\lambda ^\\star\\right)&=\\vec 0\\,,\\\\\n",
    "\\nabla_{\\vec \\lambda}\\mathcal L\\left(\\vec x ^\\star, \\vec \\lambda ^\\star\\right)&=\\vec 0\\,,\n",
    "\\end{align}\n",
    "So the FONC for $\\vec{x}^{\\star}$ to be a solution of the quadratic\n",
    "problem yields a vector $\\vec{\\lambda}^{\\star}$ such\n",
    "that the following system of equations is satisfied:\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}Q & A^\\mathsf{T}\\\\\n",
    "A & 0\n",
    "\\end{pmatrix}\\begin{pmatrix}\\vec{x}^{\\star}\\\\\n",
    "\\vec{\\lambda}^{\\star}\n",
    "\\end{pmatrix}=\\begin{pmatrix}\\vec{c}\\\\\n",
    "\\vec{b}\n",
    "\\end{pmatrix}\\,.\\label{eq:equality_constraint}\n",
    "\\end{equation}\n",
    "This system can be solved directly by factorization. An alternative\n",
    "is to use an iterative method.\n",
    "\n",
    "Consider the quadratic programming problem\n",
    "\\begin{align*}\n",
    "\\min_{\\vec{x}}\\, & \\frac{1}{2} \\vec{x}^\\mathsf{T}\\begin{pmatrix}6 & 2 & 1\\\\\n",
    "2 & 5 & 2\\\\\n",
    "1 & 2 & 4\n",
    "\\end{pmatrix}\\vec{x}- \\begin{pmatrix}8\\\\\n",
    "3\\\\\n",
    "3\n",
    "\\end{pmatrix}^\\mathsf{T}\\vec{x}\\\\\n",
    "\\textrm{subject to}\\, & \\begin{pmatrix}1 & 0 & 1\\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}\\vec{x}=\\begin{pmatrix}3\\\\\n",
    "0\n",
    "\\end{pmatrix}\\,.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [6 2 1\n",
    "     2 5 2\n",
    "     1 2 4]\n",
    "c = [8, 3, 3]\n",
    "A = [1 0 1\n",
    "     0 1 1]\n",
    "b = [3, 0]\n",
    "sol = [Q transpose(A)\n",
    "       A zeros(2,2)] \\ [c; b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution $\\vec{x}^{\\star}$ and optimal Lagrange multiplier vector $\\vec \\lambda^\\star$ are given by\n",
    "\\begin{align*}\n",
    "\\vec{x}^{\\star} & = \\begin{pmatrix}2 & -1 & 1\\end{pmatrix}^\\mathsf{T}\\,,\\\\\n",
    "\\vec{\\lambda}^{\\star} & = \\begin{pmatrix}3 & -2\\end{pmatrix}^\\mathsf{T}\\,.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Set Method\n",
    "\n",
    "We now describe active-set methods for solving quadratic programs\n",
    "containing both equality and inequality constraints. We consider only\n",
    "the convex case, in which the matrix $Q$ is positive semidefinite.\n",
    "\n",
    "If the contents of the optimal active set $J\\left(\\vec x^\\star\\right)$ were known in advance, we could find the solution $\\vec{x}^{\\star}$\n",
    "by applying the technique for equality constrained quadratic programs\n",
    "to the problem\n",
    "\\begin{align*}\n",
    "\\min_{\\vec{x}}\\, & f\\left(\\vec{x}\\right)\\overset{\\vartriangle}{=}\\frac{1}{2} \\vec{x}^\\mathsf{T}Q\\vec{x}- \\vec{c}^\\mathsf{T}\\vec{x}\\\\\n",
    "\\textrm{subject to}\\, & \\begin{cases}\n",
    "A_{\\textrm{eq}}\\vec{x}=\\vec{b}_{\\textrm{eq}}\\,,\\\\\n",
    " \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{x}=b_{\\textrm{in},j}\\,, & \\forall j\\in J\\left(\\vec{x}^{\\star}\\right)\\,,\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "where $\\vec{a}_{\\textrm{in},j}$ is the $j$th row in the matrix $A_{\\textrm{in}}$\n",
    "and $b_{\\textrm{in},j}$ is the $j$th element of the vector $\\vec{b}_{\\textrm{in}}$.\n",
    "Of course, we usually do not have prior knowledge of $J\\left(\\vec{x}^{\\star}\\right)$\n",
    "and determination of this set is the main challenge facing algorithms\n",
    "for inequality-constrained quadratic programs.\n",
    "\n",
    "Active-set methods find a step from one iterate to the next by solving\n",
    "a quadratic subproblem in which some of the inequality constraints,\n",
    "and all the equality constraints are imposed as equalities. This subset\n",
    "is referred to as the _working set_ and is denoted at the $k$th\n",
    "iterate by $W_{k}$. An important requirement we impose on $W_{k}$\n",
    "is that the gradients $\\vec{a}_{\\textrm{eq},i}$, $i=1,\\dots,m$ and\n",
    "$\\vec{a}_{\\textrm{in},j}$, $j\\in W_{k}$ are linearly independent,\n",
    "even when the full set of active constraints at that point has linearly\n",
    "dependent gradients.\n",
    "\n",
    "Given an iterate $\\vec{x}^{\\left(k\\right)}$ and the working set $W_{k}$,\n",
    "we first check whether $\\vec{x}^{\\left(k\\right)}$ minimizes the quadratic\n",
    "function $f$ in the subspace defined by the working set. If not,\n",
    "we compute a step $\\vec{d}^{\\left(k\\right)}$ by solving an equality-constrained\n",
    "quadratic subproblem in which the inequality constraints corresponding\n",
    "to the working set $W_{k}$ are regarded as equalities and the other\n",
    "inequality constraints are temporarily disregarded. To express this\n",
    "subproblem in terms of the step $\\vec{d}^{\\left(k\\right)}$, we define\n",
    " \\begin{equation}\n",
    "\\vec{d}^{\\left(k\\right)}=\\vec{x}^{\\left(k+1\\right)}-\\vec{x}^{\\left(k\\right)}\\,,\\quad\\vec{g}_{k}=Q\\vec{x}^{\\left(k\\right)}-\\vec{c}\\,.\n",
    " \\end{equation}\n",
    "By substituting for $\\vec{x}^{\\left(k+1\\right)}$ into the objective\n",
    "function, we find that\n",
    " \\begin{equation}\n",
    "f\\left(\\vec{x}^{\\left(k+1\\right)}\\right)=f\\left(\\vec{x}^{\\left(k\\right)}+\\vec{d}^{\\left(k\\right)}\\right)=\\frac{1}{2}\\left(\\vec{d}^{\\left(k\\right)}\\right)^\\mathsf{T}Q\\vec{d}^{\\left(k\\right)}+\\vec{g}_{k}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}+\\rho_{k}\\,,\n",
    " \\end{equation}\n",
    "where $\\rho_{k}=\\frac{1}{2} \\left(\\vec{x}^{\\left(k\\right)}\\right)^\\mathsf{T}Q\\vec{x}^{\\left(k\\right)}- \\vec{c}^\\mathsf{T}\\vec{x}^{\\left(k\\right)}$\n",
    "is independent of $\\vec{d}^{\\left(k\\right)}$. Since we can drop $\\rho_{k}$\n",
    "from the objective function without changing the solution of the problem,\n",
    "we can write the subproblem to be solved at the $k$th iteration as\n",
    "follows\n",
    "\\begin{align}\n",
    "\\min_{\\vec{d}^{\\left(k)\\right)}}\\, & \\frac{1}{2} \\left(\\vec{d}^{\\left(k\\right)}\\right)^\\mathsf{T}Q\\vec{d}^{\\left(k\\right)}+ \\vec{g}_{k}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}\\label{eq:quadratic_subproblem}\\\\\n",
    "\\textrm{subject to}\\, & \\begin{cases}\n",
    "A_{\\textrm{eq}}\\vec{d}^{\\left(k\\right)}=\\vec{0}\\,,\\\\\n",
    " \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}=0\\,, & \\forall j\\in W_{k}\\,.\n",
    "\\end{cases}\\nonumber \n",
    "\\end{align}\n",
    "Note that for each $j\\in W_{k}$, the value of $ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{x}^{\\left(k\\right)}$\n",
    "does not change as we move along $\\vec{d}^{\\left(k\\right)}$, since\n",
    "we have $ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\left(\\vec{x}^{\\left(k\\right)}+\\alpha\\vec{d}^{\\left(k\\right)}\\right)= \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{x}^{\\left(k\\right)}=b_{\\textrm{in},j}$\n",
    "for all $\\alpha$. Since the constraints in $W_{k}$ were satisfied\n",
    "at $\\vec{x}^{\\left(k\\right)}$, they are also satisfied at $\\vec{x}^{\\left(k\\right)}+\\alpha\\vec{d}^{\\left(k\\right)}$,\n",
    "for any value of $\\alpha$.\n",
    "\n",
    "Supposing for the moment that the optimal $\\vec{d}^{\\left(k\\right)}$\n",
    "is nonzero, we need to decide how far to move along this direction.\n",
    "If $\\vec{x}^{\\left(k\\right)}+\\vec{d}^{\\left(k\\right)}$ is feasible\n",
    "with respect to all the constraints, we set $\\vec{x}^{\\left(k+1\\right)}=\\vec{x}^{\\left(k\\right)}+\\vec{d}^{\\left(k\\right)}$.\n",
    "Otherwise, we set\n",
    " \\begin{equation}\n",
    "\\vec{x}^{\\left(k+1\\right)}=\\vec{x}^{\\left(k\\right)}+\\alpha_{k}\\vec{d}^{\\left(k\\right)}\\,,\n",
    " \\end{equation}\n",
    "where the step-length parameter $\\alpha_{k}$ is chosen to be the\n",
    "largest value in the range $\\left[0,1\\right]$ for which all constraints\n",
    "are satisfied. We can derive an explicit definition of $\\alpha_{k}$\n",
    "by considering what happens to the constraints $j\\notin W_{k}$, since\n",
    "the constraints $j\\in W_{k}$ will certainly be satisfied regardless\n",
    "of the choice of $\\alpha_{k}$. If $ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}\\leq0$\n",
    "for some $j\\notin W_{k}$, then for all $\\alpha_{k}\\geq0$, we have\n",
    "$ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\left(\\vec{x}^{\\left(k\\right)}+\\alpha_{k}\\vec{d}^{\\left(k\\right)}\\right)\\leq \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{x}^{\\left(k\\right)}\\leq b_{\\textrm{in},j}$.\n",
    "Hence, constraint $j$ will be satisfied for all nonnegative choices\n",
    "of the step-length parameter. Whenever $ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}>0$\n",
    "for some $j\\notin W_{k}$, however, we have that $ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\left(\\vec{x}^{\\left(k\\right)}+\\alpha_{k}\\vec{d}^{\\left(k\\right)}\\right)\\leq b_{\\textrm{in},j}$\n",
    "only if\n",
    " \\begin{equation}\n",
    "\\alpha_{k}\\leq\\frac{b_{\\textrm{in},j}- \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{x}^{\\left(k\\right)}}{ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}}\\,.\n",
    " \\end{equation}\n",
    "To maximize the decrease in $f$, we want $\\alpha_{k}$ to be as large\n",
    "as possible in $\\left[0,1\\right]$ subject to retaining feasibility,\n",
    "so we obtain the following definition\n",
    " \\begin{equation}\n",
    "\\alpha_{k}\\overset{\\textrm{def}}{=}\\min\\left\\{ 1,\\min_{j\\notin W_{k}, \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}>0}\\frac{b_{\\textrm{in},j}- \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{x}^{\\left(k\\right)}}{ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}}\\right\\} \\,.\n",
    " \\end{equation}\n",
    "We call the constraints $j$ for which this minimum is achieved the\n",
    "_blocking constraints_. Note that it is quite possible for $\\alpha_{k}$\n",
    "to be zero, because we could have $ \\vec{a}_{\\textrm{in},j}^\\mathsf{T}\\vec{d}^{\\left(k\\right)}>0$\n",
    "for some constraint $j$ that is active at $\\vec{x}^{\\left(k\\right)}$\n",
    "but not a member of the current working set $W_{k}$.\n",
    "\n",
    "If $\\alpha_{k}<1$, that is, the step along $\\vec{d}_{k}$ was blocked\n",
    "by some constraint not in $W_{k}$, a new working set $W_{k+1}$ is\n",
    "constructed by adding one of the blocking constraints to $W_{k}$.\n",
    "We continue to iterate in this manner, adding constraints to the working\n",
    "set until the subproblem has solution $\\vec{d}^{\\circ}=\\vec{0}$.\n",
    "Since $\\vec{d}^{\\circ}=\\vec{0}$ satisfy the optimality condition, we have that\n",
    " \\begin{equation}\n",
    "\\sum_{i=1}^{m}\\lambda_{i}^{\\circ}\\vec{a}_{\\textrm{eq},i}+\\sum_{j\\in W^{\\circ}}\\mu_{j}^{\\circ}\\vec{a}_{\\textrm{in},j}=-\\vec{g}^{\\circ}=-Q\\vec{x}^{\\circ}+\\vec{c}\\,,\n",
    " \\end{equation}\n",
    "for some Lagrange multipliers $\\lambda_{i}^{\\circ}$, $i=1,\\dots,m$\n",
    "and $\\mu_{j}^{\\circ}$, $j\\in W^{\\circ}$. It follows that $\\vec{x}^{\\circ}$,\n",
    "$\\vec{\\lambda}^{\\circ}$ and $\\vec{\\mu}^{\\circ}$ satisfy the second\n",
    "KKT condition, if we define the multipliers corresponding to the inequality\n",
    "constraints not in the working set to be zero. Because of the control\n",
    "imposed on the step length, $\\vec{x}^{\\circ}$ is also feasible with\n",
    "respect to all the constraints, so the third, fourth and fifth KKT\n",
    "conditions are satisfied at this point.\n",
    "\n",
    "We now examine the signs of the KKT multipliers in the working set,\n",
    "that is, the indices $j\\in W^{\\circ}$. If these conditions are all\n",
    "nonnegative, the first KKT condition is also satisfied, so we conclude\n",
    "that $\\vec{x}^{\\circ}$ is a KKT point for the original problem. In\n",
    "fact, since $Q$ is positive semidefinite, we have that $\\vec{x}^{\\circ}$\n",
    "is a global minimum.\n",
    "\n",
    "If, on the other hand, on or more of the multipliers $\\mu_{j}^{\\circ}$,\n",
    "$j\\in W^{\\circ}$, are negative, the first KKT condition is not satisfied\n",
    "and the objective function $f$ may be decreased by dropping one of\n",
    "these constraints. Thus, we remove an index $j$ corresponding to\n",
    "one of the negative multipliers from the working set and solve a new\n",
    "subproblem for the next step. While any index $j$ for which $\\mu_{j}^{\\circ}<0$\n",
    "usually will yield in a direction $\\vec{d}$ along which the algorithm\n",
    "can make progress, the most negative multiplier is often chosen in\n",
    "practice. This choice is motived by a sensitivity analysis, which\n",
    "shows that the rate of decrease in the objective function when one\n",
    "constraint is removed, is proportional to the magnitude of the Lagrange\n",
    "multiplier for that constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Apply the active-set method to the following problem:\n",
    "\\begin{align*}\n",
    "\\min_{\\vec{x}}\\, & f\\left(\\vec{x}\\right)=\\left(x_{1}-1\\right)^{2}+\\left(x_{2}-2.5\\right)^{2}\\\\\n",
    "\\textrm{subject to}\\, & \\begin{cases}\n",
    "-x_{1}+2x_{2}-2\\leq0\\,,\\\\\n",
    "x_{1}+2x_{2}-6\\leq0\\,.\\\\\n",
    "x_{1}-2x_{2}-2\\leq0\\,,\\\\\n",
    "-x_{1}\\leq0\\,,\\\\\n",
    "-x_{2}\\leq0\\,.\n",
    "\\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LaTeXStrings\n",
    "\n",
    "Q = Float64[2 0\n",
    "     0 2]\n",
    "c = Float64[2, 5]\n",
    "A = Float64[-1  2\n",
    "      1  2\n",
    "      1 -2\n",
    "     -1  0\n",
    "      0 -1]\n",
    "b = Float64[2, 6, 2, 0, 0]\n",
    "\n",
    "x = -1:0.05:5\n",
    "y = -1:0.05:5\n",
    "z = Surface((x,y)->(0.5 .* [x y]*Q*[x;y] .- transpose(c)*[x;y] .+ 0.5 .* transpose(c)*Q*c)[1], x, y)\n",
    "contour(x, y, z, levels=35)\n",
    "plot!(x, (2 .+ x) ./ 2, linestyle=:dash, label=L\"-x_1+2x_2-2\\le 0\")\n",
    "plot!(x, (6 .- x) ./ 2, linestyle=:dash, label=L\"x_1+2x_2-6\\le 0\")\n",
    "plot!(x, (2 .- x) ./ -2, linestyle=:dash, label=L\"x_1-2x_2-2\\le 0\")\n",
    "plot!([0,2,4,2,0,0],[0,0,1,2,1,0], linewidth=2, label=\"domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer the constraints, in order, by indices $1$ through $5$.\n",
    "For this problem it is easy to determine a feasible initial point;\n",
    "say $\\vec{x}^{\\left(0\\right)}=\\begin{pmatrix}2 & 0\\end{pmatrix}\\mathsf{T}$.\n",
    "Constraints $3$ and $5$ are active at this point, and we set $W_{0}=\\left\\{ 3,5\\right\\} $.\n",
    "Note that we could just as validly have chosen $W_{0}=\\left\\{ 5\\right\\} $\n",
    "or $W_{0}=\\left\\{ 3\\right\\} $ or even $W_{0}=\\emptyset$; each choice\n",
    "would lead the algorithm to perform somewhat differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2, 0]\n",
    "g = Q*x - c\n",
    "ain = [reshape(A[3,:], 1, 2); reshape(A[5,:], 1, 2)]\n",
    "sol = [Q transpose(ain)\n",
    "       ain zeros(2,2)] \\ [-g; 0; 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\vec{x}^{\\left(0\\right)}$ lies on a vertex of the feasible\n",
    "region, it is obviously a minimizer of the objective function $f$\n",
    "with respect to the working set $W_{0}$; that is, the solution of\n",
    "the subproblem with $k=0$ is $\\vec{d}^{\\left(0\\right)}=\\vec{0}$.\n",
    "We can then find the multipliers $\\mu_{3}^{\\circ}$ and $\\mu_{5}^{\\circ}$\n",
    "associated with the active constraints. Substitution of the data from\n",
    "our problem yields\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}-1\\\\\n",
    "2\n",
    "\\end{pmatrix}\\lambda_{3}^{\\circ}+\\begin{pmatrix}0\\\\\n",
    "1\n",
    "\\end{pmatrix}\\lambda_{5}^{\\circ}=\\begin{pmatrix}2\\\\\n",
    "-5\n",
    "\\end{pmatrix}\\,,\n",
    "\\end{equation}\n",
    "which has solution $\\lambda_{3}^{\\circ}=-2$ and $\\lambda_{5}^{\\circ}=-1$. \n",
    "\n",
    "We now remove constraint $3$ from the working set, because it has\n",
    "the most negative multiplier, and set $W_{1}=\\{5\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2, 0]\n",
    "g = Q*x - c\n",
    "ain = reshape(A[5,:], 1, 2)\n",
    "sol = [Q transpose(ain)\n",
    "       ain zeros(1,1)] \\ [-g; 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin iteration $1$ by finding the solution of the subproblem for $k=1$, which is\n",
    "$\\vec{d}^{\\left(1\\right)}= \\begin{pmatrix}-1 & 0\\end{pmatrix}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sol[1:2]\n",
    "α = min(1.0, [(reshape(A[j,:], 1, 2) * d)[1,1] ≤ 0 ? 1.0 : ((b[j] .- reshape(A[j,:], 1, 2) * x) / (reshape(A[j,:], 1, 2) * d))[1,1] for j in (1,2,3,4)]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step-length formula yields $\\alpha_{1}=1$, and the new iterate\n",
    "is $\\vec{x}^{\\left(2\\right)}=\\begin{pmatrix}1 & 0\\end{pmatrix}^\\mathsf{T}$. There are no blocking constraints, so that $W_{2}=W_{1}=\\left\\{ 5\\right\\} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + α .* d\n",
    "g = Q*x - c\n",
    "ain = reshape(A[5,:], 1, 2)\n",
    "sol = [Q transpose(ain)\n",
    "       ain zeros(1,1)] \\ [-g; 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find at the start of iteration $2$ that the solution of the\n",
    "subproblem is $\\vec{d}^{\\left(2\\right)}=\\vec{0}$. We deduce that\n",
    "the Lagrange multiplier for the lone working constraint is $\\lambda_{5}^{\\circ}=-5$,\n",
    "so we drop the working set to obtain $W_{3}=\\emptyset$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Q*x - c\n",
    "sol = Q \\ (-g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration $3$ starts by solving the unconstrained problem, to obtain\n",
    "the solution $\\vec{d}^{\\left(3\\right)}=\\begin{pmatrix}0 & 2.5\\end{pmatrix}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sol[1:2]\n",
    "α = min(1.0, [(reshape(A[j,:], 1, 2) * d)[1,1] ≤ 0 ? 1.0 : ((b[j] .- reshape(A[j,:], 1, 2) * x) / (reshape(A[j,:], 1, 2) * d))[1,1] for j in (1,2,3,4,5)]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step-length formula yields a step length of $\\alpha_{3}=0.6$\n",
    "and a new iterate $\\vec{x}^{\\left(4\\right)}=\\begin{pmatrix}1 & 1.5\\end{pmatrix}^\\mathsf{T}$. There\n",
    "is a single blocking constraint (constraint $1$), so we obtain $W_{4}=\\left\\{ 1\\right\\} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + α .* d\n",
    "g = Q*x - c\n",
    "ain = reshape(A[1,:], 1, 2)\n",
    "sol = [Q transpose(ain)\n",
    "       ain zeros(1,1)] \\ [-g; 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution of the subproblem for $k=4$ is then $\\vec{d}^{\\left(4\\right)}=\\begin{pmatrix}0.4 & 0.2\\end{pmatrix}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sol[1:2]\n",
    "α = min(1.0, [(reshape(A[j,:], 1, 2) * d)[1,1] ≤ 0 ? 1.0 : ((b[j] .- reshape(A[j,:], 1, 2) * x) / (reshape(A[j,:], 1, 2) * d))[1,1] for j in (1,2,3,4,5)]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new step-length is $1$. There are no blocking constraints\n",
    "on this step, so the next working set in unchanged: $W_{5}=\\left\\{ 1\\right\\} $. The new iterate is $\\vec{x}^{\\left(5\\right)}=\\begin{pmatrix}1.4 & 1.7\\end{pmatrix}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + α .* d\n",
    "g = Q*x - c\n",
    "ain = reshape(A[1,:], 1, 2)\n",
    "sol = [Q transpose(ain)\n",
    "       ain zeros(1,1)] \\ [-g; 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we solve the subproblem for $k=5$ to obtain a solution $\\vec{d}^{\\left(5\\right)}=\\vec{0}$.\n",
    "We find a multiplier $\\mu_{1}^{\\circ}=0.8$, so we have found the\n",
    "solution. Wet set $\\vec{x}^{\\star}=\\begin{pmatrix}1.4 & 1.7\\end{pmatrix}^\\mathsf{T}$\n",
    "and terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -1:0.05:5\n",
    "y = -1:0.05:5\n",
    "z = Surface((x,y)->(0.5 .* [x y]*Q*[x;y] .- transpose(c)*[x;y] .+ 0.5 .* transpose(c)*Q*c)[1], x, y)\n",
    "contour(x, y, z, levels=35)\n",
    "plot!(x, (2 .+ x) ./ 2, linestyle=:dash, label=L\"-x_1+2x_2-2\\le 0\")\n",
    "plot!(x, (6 .- x) ./ 2, linestyle=:dash, label=L\"x_1+2x_2-6\\le 0\")\n",
    "plot!(x, (2 .- x) ./ -2, linestyle=:dash, label=L\"x_1-2x_2-2\\le 0\")\n",
    "plot!([0,2,4,2,0,0],[0,0,1,2,1,0], linewidth=2, label=\"domain\")\n",
    "plot!([2,2,1,1,1,1.4], [0,0,0,0,1.5,1.7], linewidth=2, label=\"iterates\", markershape=:circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia\n",
    "\n",
    "The package `GeneralQP` solves the quadratic programming problems using the function `solve(Q, -c, A, b, x₀)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pkg\n",
    "#pkg\"add https://github.com/oxfordcontrol/GeneralQP.jl\"\n",
    "using GeneralQP\n",
    "\n",
    "sol = GeneralQP.solve(Q, -c, A, b, Float64[2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Quadratic Programming\n",
    "\n",
    "We consider the general constrained problem\n",
    "\\begin{align*}\n",
    "\\min\\, & f\\left(\\vec{x}\\right)\\\\\n",
    "\\textrm{subject to} & \\begin{cases}\n",
    "\\vec{h}\\left(\\vec{x}\\right)=\\vec{0}\\\\\n",
    "\\vec{g}\\left(\\vec{x}\\right)\\leq\\vec{0}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$, $\\vec{h}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$,\n",
    "$m\\leq n$, and $\\vec{g}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{p}$. The idea\n",
    "behind the _sequential quadratic programming_ (SQP) approach\n",
    "is to model the general problem at the current iterate $\\vec{x}^{\\left(k\\right)}$\n",
    "by a quadratic programming subproblem, then use the minimizer of this\n",
    "subproblem to define a new iterate $\\vec{x}^{\\left(k+1\\right)}$.\n",
    "The challenge is to design the quadratic subproblem so that it yields\n",
    "a good step for the general optimization problem.\n",
    "\n",
    "We know that the extended Lagrangian function for this problem is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}\\left(\\vec{x},\\vec{\\lambda},\\vec{\\mu}\\right)=f\\left(\\vec{x}\\right)+\\vec{\\lambda}^\\mathsf{T}\\vec{h}\\left(\\vec{x}\\right)+\\vec{\\mu}^\\mathsf{T}\\vec g\\left(\\vec{x}\\right)\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Applying Newton's method to the Lagrangian function and linearizing\n",
    "both the equality and the inequality constraints yields the following\n",
    "subproblem \n",
    "\\begin{align*}\n",
    "\\min_{\\vec{d}^{\\left(k\\right)}}\\, & \\frac{1}{2}\\left(\\vec{d}^{\\left(k\\right)}\\right)^\\mathsf{T} \\mathsf{H} \\mathcal{L}\\left(\\vec{x}^{\\left(k\\right)},\\vec{\\lambda}^{\\left(k\\right)},\\vec{\\mu}^{\\left(k\\right)}\\right)\\vec{d}^{\\left(k\\right)}+ \\mathsf{D} \\mathcal{L}\\left(\\vec{x}^{\\left(k\\right)},\\vec{\\lambda}^{\\left(k\\right)},\\vec{\\mu}^{\\left(k\\right)}\\right)\\vec{d}^{\\left(k\\right)}\\\\\n",
    "\\textrm{subject to}\\, & \\begin{cases}\n",
    " \\mathsf{J}\\vec{h}\\left(\\vec{x}^{\\left(k\\right)}\\right)\\vec{d}^{\\left(k\\right)}=-\\vec{h}\\left(\\vec{x}^{\\left(k\\right)}\\right)\\\\\n",
    " \\mathsf{D} g_{j}\\left(\\vec{x}^{\\left(k\\right)}\\right)\\vec{d}^{\\left(k\\right)}=-g_{j}\\left(\\vec{x}^{\\left(k\\right)}\\right)\\,, & j\\in W_{k}\\,,\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mu_{j}^{\\left(k\\right)}=0$, for all\n",
    "$j\\notin W_{k}$. We can use the active-set method for quadratic programming\n",
    "to solve this subproblem. The new iterate is given by $\\vec{x}^{\\left(k+1\\right)}$,\n",
    "$\\vec{\\lambda}^{\\left(k+1\\right)}$, $\\vec{\\mu}^{\\left(k+1\\right)}$\n",
    "and $W_{k+1}$.\n",
    "\n",
    "If the SQP method is able to identify the optimal active set then\n",
    "it will act like a Newton method for equality-constrained optimization\n",
    "and will converge rapidly.\n",
    "\n",
    "It is also remarkable that, far from the solution, the SQP approach\n",
    "is usually able to improve the estimate of the active set and guide\n",
    "the iterates towards a solution.\n",
    "\n",
    "Non-quadratic objective functions, however, can impede progress of\n",
    "the SQP algorithm, a phenomenon known as the Maratos effect. Steps\n",
    "that make good progress toward a solution are rejected and the algorithm\n",
    "fails to converge rapidly. These difficulties can be overcome by means\n",
    "of a _second-order correction_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pkg\n",
    "#pkg\"add NLopt\"\n",
    "\n",
    "using NLopt\n",
    "\n",
    "function myfunc(x::Vector, grad::Vector)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = 0\n",
    "        grad[2] = 0.5/sqrt(x[2])\n",
    "    end\n",
    "    return sqrt(x[2])\n",
    "end\n",
    "\n",
    "function myconstraint(x::Vector, grad::Vector, a, b)\n",
    "    if length(grad) > 0\n",
    "        grad[1] = 3a * (a*x[1] + b)^2\n",
    "        grad[2] = -1\n",
    "    end\n",
    "    (a*x[1] + b)^3 - x[2]\n",
    "end\n",
    "\n",
    "opt = Opt(:LD_SLSQP, 2)\n",
    "opt.lower_bounds = [-Inf, 0.]\n",
    "opt.xtol_rel = 1e-4\n",
    "\n",
    "opt.min_objective = myfunc\n",
    "inequality_constraint!(opt, (x,g) -> myconstraint(x,g,2,0), 1e-8)\n",
    "inequality_constraint!(opt, (x,g) -> myconstraint(x,g,-1,1), 1e-8)\n",
    "\n",
    "(minf,minx,ret) = optimize(opt, [1.234, 5.678])\n",
    "numevals = opt.numevals # the number of function evaluations\n",
    "println(\"got $minf at $minx after $numevals iterations (returned $ret)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
